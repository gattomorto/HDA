



batch = 32
(2273.60009765625, 6575.453125, 4434.5, 6594.0390625) check, sp = 0.8 (17 steps) RN
(2754.4375, 7371.7109375, 5432.296875, 7390.2734375): check, sp = 0 (17 steps) RN

(7963.1875, 10810.078125, 8531.21484375, 10842.0859375) no check  sp = 0.8 (10 step) RN
(8235.8125, 11606.19921875, 9212.96875, 11638.35546875) no check, sp = 0 (24 steps) RN


batch = 32, micro = 16 MN
(2836.0625, 6400.65234375, 4797.18359375, 6502.28515625) no check, sp = 0,
(2717.037353515625, 6387.359375, 4648.69140625, 6403.45703125) no check, sp = 0.8

(1471.0, 4269.25, 3387.69921875, 4285.34765625) check, sp = 0.8
(1532.75, 4368.33203125, 3685.08203125, 4384.39453125) sp=0

(1518.8125, 4374.15234375, 4032.93359375, 4391.79296875) no check, sp = 0, micro = T
(1397.631103515625, 4277.11328125, 3934.73828125, 4294.78515625) no check, sp = 0.8, micro = T

(814.1875, 3313.65625, 2809.75390625, 3331.32421875) check, sp = 0, micro = T
(739.53759765625, 3217.50390625, 2627.20703125, 3235.171875) check, sp = 0.8, micro = T

MN 32x32
260.0 MB base
214.9 MB sparsity = 0.8
152.5 MB check
112.2 MB sparsity, check
66.8 MB sparsity, check,


questi conti sono stati fatti su gpu
batch 32
resnet
9243.2
8678.3 sparse
2410.4 sparse recompute
1807.5 sparse recompute micro

mobile
2936.6
2952.6 sparsity
1553.5 sparsity recompute
777.6 sparsity recompute micro
480 sparsity recompute micro half


49.2% fp utilization


**INTRODUCTION**
Memory is increasingly often the bottleneck when training neural network models. Despite this, techniques to lower the overall memory requirements of training have been less widely studied compared to the extensive literature on reducing the memory requirements of inference. (Sohoni)

Recently, there has been an explosion of interest around techniques to reduce the memory require ments of neural network inference. A central driver for this work is the fact that deep neural networks are notoriously parameter-hungry, typically containing several millions or even billions of parameters. (Sohoni)

Unfortunately, training is inherently more memory-intensive than inference (often by orders of magnitude), because it requires the storage of much more than just the network itself. (Sohoni)

Recent trends in deep learning suggest that this problem will continue to worsen. Larger network architectures are becoming increasingly popular; these highly overparameterized models have achieved great successes in computer vision. (Sohoni)

However, the size of neural networks is limited by the available memory on the device(s) used for training. For certain datasets, such as video and high-resolution medical images, the prohibitive memory requirements of current training algorithms often mean that smaller, less expressive models must be used or the inputs must be downsampled to a lower dimensionality, thus destroying information. (Sohoni)

Reducing the memory requirements of training would facilitate training models from scratch (or fine-tuning existing models) on memory-poor devices, for instance smartphones. This would be advantageous to privacy (data can remain on-device)  (Sohoni)

Building deeper and larger convolutional neural networks (CNNs) is a primary trend for solving major visual
recognition task. The most accurate CNNs usually have hundreds of layers and thousands of channels thus requiring computation at billions of FLOPs. This report examines the opposite extreme: pursuing the best accuracy in very limited computational budgets at tens or hundreds of MFLOPs, focusing on common mobile platforms such as drones, robots, and smartphones. (ShuffleNet)

On-device training on small devices is gaining popularity, as
it enables machine learning models to be trained and refined
directly on small and low-power devices. On-device training
offers several benefits, including the provision of personalized
services and the protection of user privacy, as user data is never
transmitted to the cloud. However, on-device training presents
additional challenges compared to on-device inference, due to
larger memory footprints and increased computing operations
needed to store intermediate activations and gradients (https://arxiv.org/pdf/2403.19076)

**TYPES OF MEMORY**
Model Memory: The model memory is simply the memory used to store the model parameters, i.e. the weights and biases of each layer in the network. Figure 1 shows that the model memory accounts for the smallest amount of memory of the three classes of memory we have described.
Optimizer Memory: Optimizer memory refers to the memory used to store the gradients and any momentum buffers during training.
In our study, the optimizer memory is always 2 to 3 times larger than the model memory, depending on the optimizer used.
For the WideResNet architecture, it is 2Ã— the model memory when using SGD with Nesterov momentum. This is because the optimizer requires one buffer to store the gradients and an additional buffer to store the momentum values.
Activation Memory: During training, the outputs, or activations, of each layer in the network are stored for reuse in the backward pass of backpropagation (as they are required to compute gradients). This is what we refer to as the forward activation memory. (We also count the network inputs as part of the forward activation memory if they need to be stored for reuse in the backward pass.) During backpropagation, we will also compute gradients with respect to these layer outputs; we refer to this as the backward activation memory. The sum of these is the activation memory. As shown in Figure 1, activations consume most of the memory during standard training of these architectures.

on device training -> privacy, personalized services

**TECHNIQUES**
We study the fundamental tradeoffs associated with using the following four memory reduction techniques during training: (1) sparsity, (2) low precision, (3) microbatching, and (4) gradient checkpointing. First, we briefly present each technique and describe the type(s) of memory it reduces.

Parlare che ci sono tecniche per salvare memoria durante l'addestramento e per salvare la memoria durante inferenza.

Sparsity:
This technique initializes the network with a fixed sparsity pattern, and after every few iterations (based on a preset schedule), the smallest-magnitude weights of the network are pruned (set to zero) and an equal number of new nonzeros are introduced throughout the network to replace them. The frequency at which to rewire and the number of parameters to rewire (specified as a fraction of the overall network size) are important hyperparameters of this procedure. By design, the total number of nonzeros in the model is constant throughout training. Gradients are only computed with respect to the nonzero parameters, which means that the gradients are also sparse.
Using sparse operations reduces the number of FLOPs, as the zero entries can be ignored. However, in general, sparsity leads to irregular memory access patterns, so it may not actually result in a speedup; the performance depends heavily on the level of sparsity and the hardware being used.
TensorFlow don't don't implement sparse convolutions, only sparse matrix multiplication.
I tried to to implement sparse convolutions using direct,  transforming the kernel into a matrix and transforming the input in a matrix, but it's very difficult to to better than transforming into a dense format, do the convolution and go back, because dense convolution are based on matrix multiplication which is extremely optimized. (Sohoni)

Deep neural networks have led to promising breakthroughs in various applications. While the performance of deep neural networks improving, the size of these usually over-parameterized models has been tremendously increasing. (Atashgahi 2020)
Lottery Ticket Hypothesis and shown that the dense structure contains sparse sub-networks that are able to reach the same accuracy when they are trained with the same initialization (Atashgahi 2020)

The traditional dense-to-sparse training paradigm (known mainly
as network pruning) offer computational benefits just in the inference phase as first, it trains a dense network in order to prune
unimportant connections and to obtain a sparsely connected neural network.
Recently, a new sparse-to-sparse training paradigm (or simply, sparse training) began to establish inside the research community,
with several studies focus on developing memory and computational e!iciency from the start by training directly
sparse neural networks from scratch. (Curci)

The algorithm (heuristic) follows a cycle of (1) pruning weights
with small magnitude, (2) redistributing weights across layers according to the mean momentumv magnitude of existing weighs, (3) growing new weights to fill in missing connections which have the highest momentum magnitude
We use the mean magnitude of momentum Mi of existing weights Wi in each layer i to estimate
how efficient the average weight in each layer is at reducing the overall error. Intuitively, we want to
take weights from less efficient layers and redistribute them to weight-efficient layers.

Before training, we initialize the network with a certain sparsity s: we initialize the network as usual
and then remove a fraction of s weights for each layer. We train the network normally and mask the
weights after each gradient update to enforce sparsity. We apply sparse momentum after each epoch.
We can break the sparse momentum into three major parts: (a) redistribution of weights, (b) pruning
weights, (c) regrowing weights. In step (a), we we take the mean of the element-wise momentum
magnitude mi that belongs to all nonzero weights for each layer i and normalize the value by the total
momentum magnitude of all layers Pk
i=0 mi. The resulting proportion is the momentum magnitude
contribution for each layer. The number of weights to be regrow in each layer is the total number of
removed weights multiplied by each layers momentum contribution: Regrowi = Total Removed Â· mi.
In step (b), we prune a proportion of p (prune rate) of the weights with the lowest magnitude for each
layer. In step (c), we regrow weights by enabling the gradient flow of zero-valued (missing) weights
which have the largest momentum magnitude.
After each epoch, we decay the prune rate in Algorithm 1 in the same way learning rates are decayed

Modern deep neural networks are typically
highly overparameterized. Pruning techniques
are able to remove a significant fraction of network parameters with little loss in accuracy (mostafa - wang)

the method dynamically changes the sparse structure of
the network during training

Dynamic sparse reparameterization was applied to all weight tensors of convolutional layers (with the exception of downsampling convolutions and the first convolutional layer acting on the input
image), while all biases and parameters of normalization
layers were kept dense

parlare dell overhead

Low Precision:

Microbatching:
As the number of activations is directly proportional to the minibatch size, the minibatch size can instead be reduced to reduce the memory usage. However, this changes the optimization properties of the training procedure, potentially necessitating significant hyperparameter tuning. Alternatively, microbatching can be used: the minibatch is split into smaller groups called microbatches, which are each run independently forward and back through the network. The gradients are accumulated in a separate buffer until the desired number of examples is processed, and only then are the parameters and momentum buffers updated. If the
examples within a batch are independent, this is mathematically equivalent to the original procedure. However, if the model contains batch normalization layers â€“ which WideResNet does â€“ the examples within a minibatch are no longer independent. Batch normalization is ubiquitous in state-of-the-art models in many domains, especially image classification; it normalizes the activations at each layer across all the examples in each minibatch. Thus, the microbatch normalization statistics will diâ†µer from the statistics of the entire minibatch, which can affect the final accuracy of the model. Indeed, the smaller the microbatch, the less accurate the normalization statistics will be as estimates of the population statistics. (Sohoni)
Letâ€™s tackle one of the big elephants in the room: why donâ€™t we simply reduce the batch size? This is usually always an option to reduce memory consumption. However, it can sometimes result in worse predictive performance since it alters the training dynamics. (pagina web)

Gradient Checkpointing:
Gradient checkpointing is an algorithm to save memory on the network activations by only storing the activations of a subset of layers, rather than those of each layer as usual. The activations that are discarded are recomputed when necessary during backpropagation. Checkpointing is particularly convenient because the outcome of the forward and
backward passes is mathematically and numerically equivalent whether or not checkpointing is used, and so there is no memory-accuracy tradeoff.

Efficient Architectures:
Efficient neural network architectures like MobileNet and SqueezeNet are designed to deliver high accuracy in computer vision tasks while drastically reducing model size and computational requirements, making them ideal for deployment on mobile devices and other resource-constrained environments. MobileNet achieves efficiency primarily through the use of depthwise separable convolutions, which split the standard convolution operation into two simpler steps: filtering each input channel separately and then combining the outputs with lightweight 1Ã—1 convolutions. This approach significantly reduces the number of parameters and computational cost compared to traditional convolutional networks.
The primary need for such efficient architectures arises from the limitations of mobile and embedded devices, which have restricted memory, processing power, and energy resources.
Efficient neural network architectures such as MobileNet and SqueezeNet are crucial for deploying deep learning models on devices with limited computational resources, such as smartphones and embedded systems. These architectures are designed to significantly reduce the number of parameters and computational cost while maintaining competitive accuracy, making them suitable for real-time applications and environments where memory and processing power are constrained (AI)

Patching:


**DATASET**


**ALTRO**
Due to limited data and computational resources, on-device
training usually focuses on transfer learning. In transfer
learning, a neural network is first pre-trained on a large-scale
dataset, such as ImageNet [115], and used as a feature extractor [116, 117, 118]. Then, only the last layer needs to be finetuned on a smaller, task-specific dataset [119, 120, 121, 122].
This approach reduces the memory footprint by eliminating
the need to store intermediate activations during training, but
due to the limited capacity, the accuracy can be poor when the
domain shift is large (https://arxiv.org/pdf/2403.19076)





