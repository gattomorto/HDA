Modify your train_model function:
Replace the manual training loop with model.fit() to leverage built-in TensorBoard integration.
Alternatively, keep your loop but add manual profiling

https://www.tensorflow.org/guide/data_performance

Data preprocessing: you may increase num_parallel_calls in Dataset map() or preprocess the data OFFLINE.

Reading data from files in advance: you may tune parameters in the following tf.data API (prefetch size, interleave cycle_length, reader buffer_size)

https://www.tensorflow.org/guide/profiler (leggi best practices)

But what tells the loop to exit? And where do we call the code being monitored? We do that in a separate thread.

tracemalloc is really awesome, but unfortunately it only accounts for memory allocated by python, so if you have some c/c++ extension that does it own allocations, tracemalloc won't report it

If you only want to look at the memory usage of an object.
from pympler import asizeof
asizeof.asizeof(my_object)




ACTIVE PRIVATE WORKING SET
*This is a subset of the private working set that represents memory pages
that have been recently accessed by the process.
Windows prioritizes keeping this memory in RAM because it's actively being used.
This metric helps identify which processes are truly "active" in terms of
memory usage.
*Quando fai malloc, questa non cresce, ma cresce commited-- solo quando tocchi,
cresce.
*Nota che è working set, dunque è in ram
*Private working set is the paged-in, or "resident", subset of Private Bytes.

WORKING SET
*Working Set = Private Working Set + Shared Working Set
*These pages are not necessarily committed by the process;
(ecco perchè working set puo essere maggiore di commit)
*the memory is accessible by the processor with no page fault exception.
Simply put, the memory is in RAM (physical memory).
*Memory in the Working Set is "physical" in the sense that it can be
addressed without a page fault; however, the standby page list is also
still physically in memory but not reported in the Working Set,
and this is why you might see the "Mem Usage" suddenly drop when you
minimize an application.
*A working set is a subset of virtual pages resident in physical memory.
*In essence, a working set is used to decide how much physical memory can be
used to avoid a lot of paging. When a page fault occurs, the limits of the
working set and the amount of free memory on the system are examined.
If necessary, the memory manager allows a process to grow to its working set
maximum. If memory is tight, Windows will replace pages in a working set
when a page fault occurs.

COMMIT (PRIVATE BYTES)x2
*Commit Size only counts private allocations.
*Commit Size is the correct column to look at when trying to ascertain memory
consumption in processes. The sad thing is that it’s not the default column
shown, and that’s why many people use the misleading active private working
set column.(web)
*Private WS should always be lower than or equal to Private Bytes (web)
*Private Bytes refer to the amount of memory that the process executable has
asked for - not necessarily the amount it is actually using.
They are "private" because they (usually) exclude memory-mapped files
(i.e. shared DLLs). But - here's the catch - they don't necessarily exclude
memory allocated by those files (hared libraries can allocate memory inside
your application module). There is no way to tell whether a change
in private bytes was due to the executable itself, or due to a linked library.(web)
*Private Bytes are a reasonable approximation of the amount of memory
your executable is using and can be used to help narrow down a list
of potential candidates for a memory leak; if you see the number growing and
growing constantly and endlessly, you would want to check that process
for a leak. This cannot, however, prove that there is or is not a leak.

keras tuner (per ottimizzare)

Second, the optimizer and gradients. Depends on the optimizer too: if you're using momentum, you need to store that for
each weight, so it'll be about the same memory usage as the weights.

As shown by your model.summary(), the output size of this layer is (None, 1751, 480, 1024). For a single image, this is a total of 1751*480*1024 pixels. As your image is likely in float32, each pixel takes 4 bytes to store. So the output of this layer requires 1751*480*1024*4 bytes, which is around 3.2 GB per image just for this layer.
If you were to change the number of filters to, say, 64, you would only need around 200 MB per image.

Most critical though for images and high resolution data are the intermediate activations during training.
The frameworks must save the activations after each later to reuse when doing the backwards pass and computing
gradients. If you have a really deep network, or really high resolution images, or a whole lot of
channels/filters per layer, then the activations are your dominant memory usage

For inference or when finetuning models, freezing certain layers can help reduce memory requirements. Pruning, on
the other hand, removes parts of the model that contribute less to the overall output, reducing the number of
parameters and saving memory.
# Freezing layers
for param in model.layer1.parameters():
    param.requires_grad = False  # Freeze layer1 of ResNet
# Pruning a layer (example using torch.nn.utils.prune)
import torch.nn.utils.prune as prune
prune.l1_unstructured(model.layer2[0], name="weight", amount=0.3)  # Prune 30% of weights in layer2

multicore training?


Leverage garbage collection by explicitly deleting large tensors and running gc.collect() to free memory.

convertire in graph

https://youtu.be/_expOfB4CYg?t=2953

