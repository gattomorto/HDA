Modify your train_model function:
Replace the manual training loop with model.fit() to leverage built-in TensorBoard integration.
Alternatively, keep your loop but add manual profiling

https://www.tensorflow.org/guide/data_performance

Data preprocessing: you may increase num_parallel_calls in Dataset map() or preprocess the data OFFLINE.

Reading data from files in advance: you may tune parameters in the following tf.data API (prefetch size, interleave cycle_length, reader buffer_size)

https://www.tensorflow.org/guide/profiler (leggi best practices)

But what tells the loop to exit? And where do we call the code being monitored? We do that in a separate thread.

tracemalloc is really awesome, but unfortunately it only accounts for memory allocated by python, so if you have some c/c++ extension that does it own allocations, tracemalloc won't report it

If you only want to look at the memory usage of an object.
from pympler import asizeof
asizeof.asizeof(my_object)




ACTIVE PRIVATE WORKING SET
*This is a subset of the private working set that represents memory pages
that have been recently accessed by the process.
Windows prioritizes keeping this memory in RAM because it's actively being used.
This metric helps identify which processes are truly "active" in terms of
memory usage.
*Quando fai malloc, questa non cresce, ma cresce commited-- solo quando tocchi,
cresce.
*Nota che è working set, dunque è in ram
*Private working set is the paged-in, or "resident", subset of Private Bytes.

WORKING SET
*Working Set = Private Working Set + Shared Working Set
*These pages are not necessarily committed by the process;
(ecco perchè working set puo essere maggiore di commit)
*the memory is accessible by the processor with no page fault exception.
Simply put, the memory is in RAM (physical memory).
*Memory in the Working Set is "physical" in the sense that it can be
addressed without a page fault; however, the standby page list is also
still physically in memory but not reported in the Working Set,
and this is why you might see the "Mem Usage" suddenly drop when you
minimize an application.
*A working set is a subset of virtual pages resident in physical memory.
*In essence, a working set is used to decide how much physical memory can be
used to avoid a lot of paging. When a page fault occurs, the limits of the
working set and the amount of free memory on the system are examined.
If necessary, the memory manager allows a process to grow to its working set
maximum. If memory is tight, Windows will replace pages in a working set
when a page fault occurs.

COMMIT (PRIVATE BYTES)x2
*Commit Size only counts private allocations.
*Commit Size is the correct column to look at when trying to ascertain memory
consumption in processes. The sad thing is that it’s not the default column
shown, and that’s why many people use the misleading active private working
set column.(web)
*Private WS should always be lower than or equal to Private Bytes (web)
*Private Bytes refer to the amount of memory that the process executable has
asked for - not necessarily the amount it is actually using.
They are "private" because they (usually) exclude memory-mapped files
(i.e. shared DLLs). But - here's the catch - they don't necessarily exclude
memory allocated by those files (hared libraries can allocate memory inside
your application module). There is no way to tell whether a change
in private bytes was due to the executable itself, or due to a linked library.(web)
*Private Bytes are a reasonable approximation of the amount of memory
your executable is using and can be used to help narrow down a list
of potential candidates for a memory leak; if you see the number growing and
growing constantly and endlessly, you would want to check that process
for a leak. This cannot, however, prove that there is or is not a leak.

keras tuner (per ottimizzare)

Second, the optimizer and gradients. Depends on the optimizer too: if you're using momentum, you need to store that for
each weight, so it'll be about the same memory usage as the weights.

As shown by your model.summary(), the output size of this layer is (None, 1751, 480, 1024). For a single image, this is a total of 1751*480*1024 pixels. As your image is likely in float32, each pixel takes 4 bytes to store. So the output of this layer requires 1751*480*1024*4 bytes, which is around 3.2 GB per image just for this layer.
If you were to change the number of filters to, say, 64, you would only need around 200 MB per image.

Most critical though for images and high resolution data are the intermediate activations during training.
The frameworks must save the activations after each later to reuse when doing the backwards pass and computing
gradients. If you have a really deep network, or really high resolution images, or a whole lot of
channels/filters per layer, then the activations are your dominant memory usage

For inference or when finetuning models, freezing certain layers can help reduce memory requirements. Pruning, on
the other hand, removes parts of the model that contribute less to the overall output, reducing the number of
parameters and saving memory.
# Freezing layers
for param in model.layer1.parameters():
    param.requires_grad = False  # Freeze layer1 of ResNet
# Pruning a layer (example using torch.nn.utils.prune)
import torch.nn.utils.prune as prune
prune.l1_unstructured(model.layer2[0], name="weight", amount=0.3)  # Prune 30% of weights in layer2

multicore training?


Leverage garbage collection by explicitly deleting large tensors and running gc.collect() to free memory.

convertire in graph

https://youtu.be/_expOfB4CYg?t=2953

model pruning:
import tensorflow_model_optimization as tfmot
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(...)
model_pruned = tfmot.sparsity.keras.prune_low_magnitude(model, pruning_schedule)

Note: It is not recommended to set this to "float16" for training, as this will likely cause numeric stability
issues. Instead, mixed precision, which leverages a mix of float16 and float32. It can be configured
by calling keras.mixed_precision.set_dtype_policy('mixed_float16'). (documentazione ufficiale)

Beware though if you try this API on CPU, as mixed precision is expected to run significantly slower on
such devices.
(a) In old GPU architectures and on most CPUs, FP16 numbers are treated as FP32, which requires casting to
FP32 before computations and recast back to FP16 after. This makes more operations than for FP32 numbers,
which explains why mixed precision is not preferable in such cases. This also means that if your GPU is
really old, you might also experience slower training when using mixed precision.

https://docs.openvino.ai/2025/index.html

policy_name = "mixed_float16"  # if you train on a GPU
policy_name = "mixed_bfloat16"  # if you train on a TPU
# With recent Tensorflow versions (>= 2.4)
policy = tf.keras.mixed_precision.Policy("mixed_float16")
tf.keras.mixed_precision.set_policy(policy)
# Before Tensorflow 2.4
policy = tf.keras.mixed_precision.experimental.Policy("mixed_float16")
tf.keras.mixed_precision.experimental.set_policy(policy)
optimizer = Adam(**your_parameters)
# For Tensorflow <2.4
scaled_optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, "dynamic")
# For Tensorflow >=2.4
scaled_optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer, "dynamic")

*nel caso non ti dovessi ricordare--   #tf.keras.backend.set_floatx('float16') riduceva la memoria di molto
ma visto che dovrà emulare
* su cpu colab mixedBfloat porta ad una riduzione di memoria
* su cpu colab mixedfloat è molto lento e non porta a riduzione
* su cpu colab float16 sembra ancora di piu ridurre memoria, ma è molto lento
* su cpu in realtà mi sembra che sono tutti lenti uguale -- ma mixedbfloat è piu veloce
(ma lento cmq)
* su gpu non ce nessun vantaggio di memoria
* su tpu di base è su 17/21 ma lento
* su tpu mixedloat float 17
*su tpu mixedbfloat 17

tf.config.optimizer.set_jit(True)

tf.keras.backend.clear_session()


ricorda che con la gpu print(tf.config.experimental.get_memory_info('GPU:0')) il peak è sempre
costante.

nella seconda versione vuole che W1 sia definito con indici ordinati--ho paura che nella prima versione
li va a riordinare da solo
hidden = tf.sparse.sparse_dense_matmul(X, self.W1)
#hidden = tf.matmul(X, tf.sparse.to_dense(self.W1))


Convert W_indices and W_shapes to TensorFlow constants during initialization to avoid repeated conversions during forward passes.


This uses sparse to dense op, which is mostly deprecated in favor of scatter_nd op. That op has a gradient.
We should change the sparse to dense Python wrapper to call scatter_nd.


Here is the solution.
open YOURPATH/tensorflow/python/ops/sparse_grad.py
Remove line 33 ops.NotDifferentiable("SparsetoDense")
At the end, append
@ops.RegisterGradient("SparseToDense")
def _SparseToDenseGrad(op, grad):
sparse_indices, output_shape, _, _ = op.inputs
sparse_values_grad = array_ops.gather_nd(grad, sparse_indices)
default_value_grad = math_ops.reduce_sum(grad) - math_ops.reduce_sum(
sparse_values_grad)
return [
array_ops.zeros_like(sparse_indices),
array_ops.zeros_like(output_shape), sparse_values_grad, default_value_grad
]
This implementation is from TF2.0 version.


E se provassi ad usare assign() su sparse tensor per modificare il valore invece di ricrearlo di nuovo?

da qualche parte uso vettoru numpy, da qualche altra parte vettrori std di python -- deve essere uniforme

https://github.com/tensorflow/tensorflow/issues/46089
https://github.com/tensorflow/tensorflow/issues/46706

sparse_softmax
sparse_dense_cwise_mul in un eventuale tua adagrad?
sparse_concat in regrow_layer?
serialize_many_sparse?


If you would like to define the tensor outside the graph, e.g. define the sparse tensor for later data feed, use SparseTensorValue. In contrast, if the sparse tensor is defined in graph, use SparseTensor

sparse_tensor_dense_mat_mul vanno bene indici int32


x = tf.keras.Input(shape=(4,), sparse=True)
y = tf.keras.layers.Dense(4)(x)
model = tf.keras.Model(x, y)
sparse_data = tf.sparse.SparseTensor(
    indices = [(0,0),(0,1),(0,2),
               (4,3),(5,0),(5,1)],
    values = [1,1,1,1,1,1],
    dense_shape = (6,4)
)
model(sparse_data)
model.predict(sparse_data)

fare anche l'output sparso?


tf.raw_ops.SparseTensorDenseMatMul


controlla se tutte le funzioni su cui si poteva fare checkoiint sono state fatte (anhe fully connected in fondo)/ batch norm

